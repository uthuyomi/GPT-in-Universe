# ==============================================================
# ğŸŒŒ AI Meaning Universe â€” main_meaning_universe.py
# æ—¥æœ¬èªã®è¤‡æ•°è³ªå• â†’ å›ç­”ç¾¤ç”Ÿæˆ â†’ TF-IDF â†’ PCA(3D) â†’ KMeans
# â†’ universe.jsonï¼ˆThree.jsç”¨ï¼‰ã¨CSVã‚’å‡ºåŠ›
# ä¾å­˜: openai, pandas, numpy, scikit-learn, matplotlib(ä»»æ„), tqdm
# ==============================================================

import os
import re
import json
from dataclasses import dataclass
from typing import List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾ç­–ï¼ˆä»»æ„ï¼‰ ---
try:
    import matplotlib.pyplot as plt  # è§£æãƒ‡ãƒãƒƒã‚°ã§ä½¿ã†ãªã‚‰
    import matplotlib.font_manager as fm
    if os.name == "nt":
        FONT_PATH = "C:/Windows/Fonts/msgothic.ttc"  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
        if os.path.exists(FONT_PATH):
            plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
            plt.rcParams["axes.unicode_minus"] = False
    try:
        import japanize_matplotlib  # noqa: F401
    except Exception:
        pass
except Exception:
    pass

# --- OpenAI SDK ---
from openai import OpenAI
client = OpenAI(api_key="")  # ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒå¿…è¦

# --- JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•° ---
# åŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯ãƒ‘ã‚¹ã‚’é€šã—ãŸä¸Šã§èª­ã¿è¾¼ã¿
from exporter_universe_json import export_universe_json


# ==========================
# è¨­å®š
# ==========================
MODEL = "gpt-4o"

# â˜…ã“ã“ã‚’ç·¨é›†ï¼šè³ªå•ãƒªã‚¹ãƒˆï¼ã‚µãƒ³ãƒ—ãƒ«æ•°
QUESTIONS: List[str] = [
    "äººé–“ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "é­‚ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "AIã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "ç”Ÿå‘½ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "æ„è­˜ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "æ™‚é–“ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "å®‡å®™ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚"
]

NUM_SAMPLES_PER_QUESTION = 15   # ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ãªã‚‰ 20ã€œ50 æ¨å¥¨
NUM_CLUSTERS = None             # è‡ªå‹•æ¨å®šï¼ˆå›ºå®šãªã‚‰æ•´æ•°ï¼‰

# å‡ºåŠ›å…ˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰è¦‹ãŸç›¸å¯¾ãƒ‘ã‚¹ã‚’æƒ³å®šï¼‰
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CSV_OUT = os.path.join(DATA_DIR, "ai_universe_dataset.csv")
JSON_OUT = os.path.join(DATA_DIR, "universe.json")

# ä¹±æ•°å›ºå®šï¼ˆå†ç¾æ€§ï¼‰
RANDOM_STATE = 42


# ==========================
# ãƒ‡ãƒ¼ã‚¿åé›†
# ==========================
def generate_answers(questions: List[str], n_per_q: int) -> pd.DataFrame:
    rows = []
    for qi, q in enumerate(questions):
        print(f"ğŸ”¹ å•ã„ {qi+1}/{len(questions)}: '{q}' ã‚’ {n_per_q} å›ç”Ÿæˆâ€¦")
        for _ in tqdm(range(n_per_q)):
            res = client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": q}],
            )
            text = res.choices[0].message.content.strip().replace("\n", " ")
            rows.append({"QID": qi, "Question": q, "Output": text})
    df = pd.DataFrame(rows)
    return df


# ==========================
# å‰å‡¦ç†ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»æ¬¡å…ƒå‰Šæ¸›ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿
# ==========================
def vectorize_and_reduce(outputs: pd.Series) -> Tuple[TfidfVectorizer, np.ndarray, np.ndarray]:
    """TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€PCAã§3æ¬¡å…ƒã«é‚„å…ƒã€‚"""
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(outputs)
    pca = PCA(n_components=3, random_state=RANDOM_STATE)
    pts3 = pca.fit_transform(X.toarray())
    return vectorizer, X, pts3


def auto_k(n_points: int) -> int:
    """ç‚¹æ•°ã‹ã‚‰ãƒ©ãƒ•ã«ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¨å®šã€‚"""
    # ã–ã£ãã‚Šï¼š15ç‚¹ã«ã¤ã1ã‚¯ãƒ©ã‚¹ã‚¿ã€6ã€œ16ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
    k = int(np.clip(np.round(n_points / 15), 6, 16))
    return k


def cluster_points(X, num_clusters: int | None) -> Tuple[KMeans, np.ndarray]:
    if num_clusters is None:
        k = auto_k(X.shape[0])
    else:
        k = int(num_clusters)
    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init="auto")
    labels = kmeans.fit_predict(X)
    return kmeans, labels


def cluster_top_terms(vectorizer: TfidfVectorizer, kmeans: KMeans, topn: int = 5) -> List[str]:
    """å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½èªã‚’â€˜ãƒ»â€™ã§é€£çµã—ã¦è¿”ã™ã€‚"""
    terms = np.array(vectorizer.get_feature_names_out())
    order = kmeans.cluster_centers_.argsort()[:, ::-1]
    out = []
    for i in range(order.shape[0]):
        top = terms[order[i, :topn]]
        out.append("ãƒ»".join(top))
    return out


# ==========================
# è¦ç´„ï¼ˆå…¨ä½“ï¼‰
# ==========================
def summarize_all(df: pd.DataFrame, lang: str = "ja") -> str:
    joined = "\n".join(df["Output"].tolist())

    if lang == "ja":
        prompt = f"""
æ¬¡ã®AIã®å›ç­”ç¾¤ã¯ã€è¤‡æ•°ã®å“²å­¦çš„å•ã„ï¼ˆä¾‹ï¼š{ 'ã€'.join(QUESTIONS[:3]) } â€¦ï¼‰ã«å¯¾ã™ã‚‹ã‚‚ã®ã§ã™ã€‚
ã“ã®é›†åˆã‹ã‚‰ã€AIãŒãã‚Œã‚‰ã®æ¦‚å¿µã‚’ã©ã®ã‚ˆã†ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ã‚’
â‘ å…±é€šç‚¹ â‘¡ç›¸é•ç‚¹ â‘¢åˆ†å¸ƒå‚¾å‘ï¼ˆã©ã®æ–¹å‘ã«è§£é‡ˆãŒå¯„ã‚‹ã‹ï¼‰
ã®3é …ç›®ã§ã€å¹³æ˜“ãªæ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
ã€å›ç­”ç¾¤ã€‘
{joined}
"""
    else:
        prompt = f"""
These are AI-generated answers to multiple philosophical questions
(e.g., {"; ".join(QUESTIONS[:3])} â€¦).
Please summarize how the AI seems to understand these concepts in three parts:
(1) Common points, (2) Differences, and (3) Overall tendencies of interpretation.
Use simple, clear English.
[Outputs]
{joined}
"""
    res = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
    )
    return res.choices[0].message.content.strip()


# ==========================
# ãƒ¡ã‚¤ãƒ³
# ==========================
def main():
    os.makedirs(DATA_DIR, exist_ok=True)

    # 1) åé›†
    df = generate_answers(QUESTIONS, NUM_SAMPLES_PER_QUESTION)
    df.to_csv(CSV_OUT, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ CSV: {CSV_OUT}")

    # 2) ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‹PCA(3D)
    vectorizer, X, pts3 = vectorize_and_reduce(df["Output"])

    # 3) ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    kmeans, labels = cluster_points(X, NUM_CLUSTERS)
    df["Cluster"] = labels

    # 4) ä»£è¡¨èª
    top_terms = cluster_top_terms(vectorizer, kmeans, topn=5)
    print("ğŸ§  å„ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨èª:")
    for i, t in enumerate(top_terms):
        print(f"  C{i}: {t}")

    # 5) å…¨ä½“è¦ç´„ï¼ˆä»»æ„ï¼Three.jsã«ã¯ä¸è¦ã ãŒãƒ­ã‚°ã¨ã—ã¦ï¼‰
    try:
        summary_ja = summarize_all(df, lang="ja")
        summary_en = summarize_all(df, lang="en")
        with open(os.path.join(DATA_DIR, "summary_ja.txt"), "w", encoding="utf-8") as f:
            f.write(summary_ja)
        with open(os.path.join(DATA_DIR, "summary_en.txt"), "w", encoding="utf-8") as f:
            f.write(summary_en)
        print("ğŸ’¾ summaries: summary_ja.txt / summary_en.txt")
    except Exception as e:
        print(f"è¦ç´„ã¯ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # 6) Three.jsç”¨ã® universe.json ã‚’å‡ºåŠ›
    export_universe_json(
        outfile=JSON_OUT,
        points3d=pts3,
        labels=labels,
        df=df[["Question", "Output"]],
        cluster_terms=top_terms,
        questions=QUESTIONS
    )
    print(f"âœ… å®Œäº†: {JSON_OUT}")


if __name__ == "__main__":
    main()
    
    
